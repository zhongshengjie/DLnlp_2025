{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8212f71a-3313-4ce5-8df0-e6ae144ca307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# === 数据清洗与加载 ===\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\" \n",
    "def load_corpus(data_dir):\n",
    "    \"\"\"加载金庸小说语料库\"\"\"\n",
    "    texts = []\n",
    "    for fp in os.listdir(data_dir):\n",
    "        if fp.endswith('.txt'):\n",
    "            with open(os.path.join(data_dir, fp), 'r', encoding='gb18030') as f:\n",
    "                text = re.sub(r'[\\r\\t\\u3000]', '', f.read())  # 去除全角空格/制表符\n",
    "                text = re.sub(r'\\n{2,}', '\\n', text)          # 合并多余空行\n",
    "                texts.append(text)\n",
    "    return '\\n'.join(texts)[:500000]  # 限制语料规模以适应显存\n",
    "\n",
    "# === 字符级处理（LSTM用）===\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"字符级数据集\"\"\"\n",
    "    def __init__(self, text, block_size=128):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}  # 0为padding\n",
    "        self.itos = {i:ch for ch,i in self.stoi.items()}\n",
    "        self.ids = [self.stoi[ch] for ch in text]\n",
    "        self.samples = [self.ids[i:i+block_size] \n",
    "                       for i in range(0, len(self.ids)-block_size, block_size//2)]\n",
    "        \n",
    "    def __len__(self): return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.samples[idx]\n",
    "        x = torch.tensor(seq[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(seq[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# === 子词处理（Transformer用）===\n",
    "class GPT2Dataset(Dataset):\n",
    "    \"\"\"改进后的分块逻辑\"\"\"\n",
    "    def __init__(self, text, tokenizer, block_size=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        # 增加分块前的截断处理（参考网页6）\n",
    "        encoded = tokenizer.encode(text, max_length=block_size*1000, truncation=True)\n",
    "        # 使用滑动窗口分块（参考网页8）\n",
    "        self.samples = [encoded[i:i+block_size] \n",
    "                       for i in range(0, len(encoded)-block_size, block_size//2)]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.samples[idx]\n",
    "        return torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "# 初始化Tokenizer\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "text = \"郭靖站起身来，只见远处黄沙漫天，隐约传来马蹄声响\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "# 强制截断到最大长度(512)\n",
    "inputs = tokenizer(text, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # 添加缺失的pad_token\n",
    "# 数据加载示例\n",
    "corpus = load_corpus('C:\\文件\\小说数据集')\n",
    "lstm_ds = CharDataset(corpus)\n",
    "gpt2_ds = GPT2Dataset(corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63ea21f-d8b1-4ffa-945a-105cf5279ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 6.168\n",
      "Epoch 1 Loss: 6.435\n",
      "Epoch 2 Loss: 6.573\n",
      "Epoch 3 Loss: 6.165\n",
      "Epoch 4 Loss: 6.143\n",
      "Epoch 5 Loss: 5.639\n",
      "Epoch 6 Loss: 6.150\n",
      "Epoch 7 Loss: 5.307\n",
      "Epoch 8 Loss: 5.874\n",
      "Epoch 9 Loss: 5.383\n"
     ]
    }
   ],
   "source": [
    "class CharLSTM(torch.nn.Module):\n",
    "    \"\"\"基于字符的LSTM生成模型\"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim=256, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = torch.nn.LSTM(emb_dim, hidden_size, num_layers=3, \n",
    "                                 dropout=0.2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        return self.fc(out), hidden\n",
    "\n",
    "# 训练配置\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lstm_model = CharLSTM(len(lstm_ds.stoi)+1).to(device)\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 数据加载器\n",
    "lstm_dl = DataLoader(lstm_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    lstm_model.train()\n",
    "    for x, y in lstm_dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = lstm_model(x)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch} Loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6481a7a8-4c6a-4ef3-9bbb-8246988cdb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu128\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [186/186 00:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.477000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=186, training_loss=3.6838741507581485, metrics={'train_runtime': 42.8807, 'train_samples_per_second': 139.783, 'train_steps_per_second': 4.338, 'total_flos': 772901830656000.0, 'train_loss': 3.6838741507581485, 'epoch': 2.96})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "print(torch.__version__)        # 需≥2.5.1\n",
    "print(torch.cuda.is_available()) # 需返回True\n",
    "# 模型初始化\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-lyric\")\n",
    "gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# 自定义数据整理函数\n",
    "def collate_fn(batch):\n",
    "    return {'input_ids': torch.stack(batch), \n",
    "            'labels': torch.stack(batch)}\n",
    "\n",
    "# 训练器\n",
    "trainer = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args,\n",
    "    train_dataset=gpt2_ds,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "# 开始微调\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ecaa431-a311-4838-baa5-f66134eb37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LSTM生成结果 ===\n",
      "郭靖站起身来狱，就是心刀，忽乾在是不北举开。头来又伸来出，一一酸的拉边。\n",
      "李家洛想着言喜空头，实头已住，将手一前，我一台一骂，便是不意，过中疾惊，徒等撞钗的这口，不年上，帮方必了，听是是方起？”公过和道来中上，小心探过，待《长里着这语，性兵一住。那觉两人向他伤子，又不个说，见家道：“那时卫势这事在香他答，…用好截抱去，听讥对说在那旗说兵成门，会不在东，见我来辈兴功。”\n",
      "陈家洛见一过一有两个齐来，要道：“俯里红来。不哥有的奔成。这人不么，不得好。”他哥》棋红，面目对看。\n",
      "又是在健世在这出了。周菲仲指想上不生，在道：“我你我教便意。”他说：“总是放了。”天亲万叫：“咱要要，是我要想，”这人有不机之了，他把他来在不么，挺摇寡花，张这日大，要家大南？赵她用台火里一大，说得说拉，有是对四了打过，不位之声，不么，就若冲，然想到说来，大人在不借多上。”周沅洛道：“只想来就要回有，只然你来心敛，怎下一亲祭的，你兄么后拚儿见，有是率交蛋人的语。”徐绮桐道：“我果不能再去。那敢是逼叫便包度外，在这么你的我打层。”周菲青桐道：“我镖。”总卓同一起来。\n",
      "陈家洛道：“各人要口，大然不人。他让连说“三么，我凉不有...\n",
      "\n",
      "=== GPT-2生成结果 ===\n",
      "郭 靖 站 起 身 来 ， ， 大 骆 驼 ， 骆 驼 越 来 越 近 。 他 的 手 脚 并 非 短 小 ， 但 轻 便 易 举 ， 一 旦 发 觉 自 己 有 一 腿 ， 越 是 怕 难 ， 就 别 再 给 他 手 脚 伤 了 。 骆 驼 手 的 眼 珠 ， 不 免 被 他 夺 了 半 点 ， 那 又 足 够 了 。 ， 白 龙 马 伸 出 马 尾 ， 说 道 ： 宝 贝 ， 我 们 一 定 尽 快 去 吧 。 骆 驼 上 树 又 下 河 ， 马 头 飞 起 ， 风 急 雨 急 ， 他 手 脚 并 着 ， 在 湖 泊 中 上 下 窜 来 窜 去 ， 心 想 他 是 一 个 英 雄 好 汉 ， 如 今 可 不 得 让 他 为 难 ， 那 是 不 能 ， 不 然 要 是 真 抢 到 了 他 手 脚 甚 么 。 骆 驼 手 中 的 缰 绳 ， 一 旦 要 去 ， 他 马 上 翻 身 ， 手 脚 一 横 ， 越 想 越 。 他 又 一 回 头 ， 眼 望 大 抬 头 望 去 ， 望 得 是 一 片 旷 野 ， 忽 听 掌 声 之 间 ， 眼 见 人 影 无 声 的 闪 过 ， 是 那 马 ...\n"
     ]
    }
   ],
   "source": [
    "def generate_lstm(model, start_text, length=500, temperature=0.8):\n",
    "    \"\"\"LSTM逐字符生成\"\"\"\n",
    "    model.eval()\n",
    "    chars = [lstm_ds.stoi.get(ch, 0) for ch in start_text]\n",
    "    hidden = None\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            x = torch.tensor([chars[-1]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "            logits, hidden = model(x, hidden)\n",
    "            prob = torch.softmax(logits[0,-1]/temperature, dim=-1)\n",
    "            next_char = torch.multinomial(prob, 1).item()\n",
    "            chars.append(next_char)\n",
    "    return ''.join([lstm_ds.itos.get(c, '') for c in chars])\n",
    "\n",
    "def generate_gpt2(model, start_text, length=500, temperature=0.9):\n",
    "    \"\"\"GPT-2生成\"\"\"\n",
    "    input_ids = tokenizer.encode(start_text, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=len(input_ids[0])+length,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 生成示例对比\n",
    "lstm_output = generate_lstm(lstm_model, start_text=\"郭靖站起身来\")\n",
    "gpt2_output = generate_gpt2(gpt2_model, start_text=\"郭靖站起身来\")\n",
    "\n",
    "print(\"=== LSTM生成结果 ===\")\n",
    "print(lstm_output[:500] + \"...\")\n",
    "print(\"\\n=== GPT-2生成结果 ===\")\n",
    "print(gpt2_output[:500] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
